######################### AI ###########################################################
@article{Buchanan,
  title={A (Very) Brief History of Artificial Intelligence},
  author={Bruce G. Buchanan},
  journal={AI Magazine},
  year={2005},
  volume={26},
  pages={53-60}
}

@article{Connell,
  title={Four Paths to AI},
  author={Connell, Jonathan and Livingston, Kenneth},
  journal={Frontiers in artificial intelligence and applications},
  volume={171},
  pages={394},
  year={2008},
  publisher={IOS Press}
}

@article{Gugerty,
author = {Gugerty, Leo},
year = {2006},
month = {10},
pages = {880-884},
title = {Newell and Simon's Logic Theorist: Historical Background and Impact on Cognitive Modeling},
volume = {50},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
doi = {10.1177/154193120605000904}
}

@book{Jones,
 author = {Jones, Tim},
 title = {Artificial Intelligence: A Systems Approach with CD},
 year = {2008},
 isbn = {0763773379, 9780763773373},
 edition = {1st},
 publisher = {Jones and Bartlett Publishers, Inc.},
 address = {USA},
} 

@incollection{Searle,
 author = {Searle, John R.},
 chapter = {Minds, Brains, and Programs},
 title = {Mind Design},
 editor = {Haugeland, John},
 year = {1985},
 isbn = {0-262-58052-7},
 pages = {282--307},
 numpages = {26},
 url = {http://dl.acm.org/citation.cfm?id=42462.42463},
 acmid = {42463},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{Turing,
    author = {TURING, A. M.},
    title = "{I.â€”COMPUTING MACHINERY AND INTELLIGENCE}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://dx.doi.org/10.1093/mind/LIX.236.433},
    eprint = {http://oup.prod.sis.lan/mind/article-pdf/LIX/236/433/9866119/433.pdf},
}


######################### Applications ##################################################

@mastersthesis{Evaluation,
  title={Evaluation of Hierarchical Temporal Memory in algorithmic trading},
  author={{\AA}slin, Fredrik},
  school={ Institutionen f\"{o}r Datavetenskap},
  year={2010},
  month={2}  
}

@inproceedings{EvolvingTrading,
 author = {Gabrielsson, Patrick and K\"{o}nig, Rikard and Johansson, Ulf},
 title = {Evolving Hierarchical Temporal Memory-based Trading Models},
 booktitle = {Proceedings of the 16th European Conference on Applications of Evolutionary Computation},
 series = {EvoApplications'13},
 year = {2013},
 isbn = {978-3-642-37191-2},
 location = {Vienna, Austria},
 pages = {213--222},
 numpages = {10},
 url = {http://dx.doi.org/10.1007/978-3-642-37192-9_22},
 doi = {10.1007/978-3-642-37192-9_22},
 acmid = {2456641},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@mastersthesis{Galetzka,
  title={Intelligent Predictions: an Empirical Study of the Cortical Learning Algorithm},
  author={Galetzka, Michael},
  school={University of Applied Sciences Mannheim},
  year={2014}
}

@mastersthesis{WeatherForecast,
    title={Utilizing the HTM algorithms for weather forecasting and anomaly detection},
    author={Vivmond, Alexandre},
    school={University of Bergen},
    year={2016}
}


######################### Computational Neuroscience ####################################

@book{ComputationalExpolorations,
 author = {O'Reilly, Randall C. and Munakata, Yuko},
 title = {Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain},
 year = {2000},
 isbn = {0262650541},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


######################### Math ##########################################################

@book{Adams,
  author    = {Adams, C and Franzosa, Robert}, 
  title     = {Introduction to Topology: Pure and Applied},
  publisher = {Pearson Prentice Hall},
  year      = 2008,
  address   = {Upper Saddle River, NY},
  isbn      = {9780131848696}
}

@book{Box,
  author    = {Box, George E. P and Jenkins, Gwilym M.}, 
  title     = {Time Series Analysis: Forecasting and Control},
  publisher = {John Wiley \& Sons, Inc.},
  year      = 2016,
  address   = {Hoboken, N.J.},
  edition   = 5,
  isbn      = {9781118675021}
}

@book{Munkres,
 author = {Munkres, J},
 title = {Topology},
 year = {2000},
 isbn = {9780131816299},
 edition = {2},
 publisher = {Prentice Hall},
 address = {Upper Saddle River, NJ}
}

######################### Numenta #######################################################
@ONLINE{DiscoveriesBrainWorks,
    title = {Numenta Research: Key Discoveries in Understanding How the Brain Works},
        year = {2018},
        month = {August},
        organization = {Youtube},
        author = {Numenta},
        url = {https://www.youtube.com/watch?v=X50GY0mdHlw&t},
}

@ARTICLE{HowDoNeurons,
   author = {{Ahmad}, S. and {Hawkins}, J.},
    title = "{How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1601.00720},
 primaryClass = "q-bio.NC",
 keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Artificial Intelligence},
     year = 2016,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160100720A}
}


@article{Towards,
    author = {George, Dileep AND Hawkins, Jeff},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Towards a Mathematical Theory of Cortical Micro-circuits},
    year = {2009},
    month = {10},
    volume = {5},
    url = {https://doi.org/10.1371/journal.pcbi.1000532},
    pages = {1-26},
    abstract = {Author Summary Understanding the computational and information processing roles of cortical circuitry is one of the outstanding problems in neuroscience. In this paper, we work from a theory of neocortex that models it as a spatio-temporal hierarchical system to derive a biological cortical circuit. This is achieved by combining the computational constraints provided by the inference equations for this spatio-temporal hierarchy with anatomical data. The result is a mathematically consistent biological circuit that can be mapped to the cortical laminae and matches many prominent features of the mammalian neocortex. The mathematical model can serve as a starting point for the construction of machines that work like the brain. The resultant biological circuit can be used for modeling physiological phenomena and for deriving testable predictions about the brain.},
    number = {10},
    doi = {10.1371/journal.pcbi.1000532}
}

@unpublished{BAMI,
   title={Biological and Machine Intelligence (BAMI)},
   author={Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
   note={Initial online release 0.4},
   url={https://numenta.com/resources/biological-and-machine-intelligence/},
   year={2016}
}

@ARTICLE{TheoryOfHow,
  
AUTHOR={Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},   
	 
TITLE={A Theory of How Columns in the Neocortex Enable Learning the Structure of the World},      
	
JOURNAL={Frontiers in Neural Circuits},      
	
VOLUME={11},      

PAGES={81},     
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncir.2017.00081},       
	
DOI={10.3389/fncir.2017.00081},      
	
ISSN={1662-5110},   
   
ABSTRACT={Neocortical regions are organized into columns and layers.  Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.}
}

@article{Whitepaper,
    author = {Hawkins, Jeff and Ahmad, Subutai and Dubinsky, Donna},
    publisher = {Numenta},
    title = {Hierarchical Temporal Memory including HTM Cortical Learning Algorithms},
    year = {2011}
}

@ARTICLE{WhyNeuronsHave,
  
AUTHOR={Hawkins, Jeff and Ahmad, Subutai},   
	 
TITLE={Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},      
	
JOURNAL={Frontiers in Neural Circuits},      
	
VOLUME={10},      

PAGES={23},     
	
YEAR={2016},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncir.2016.00023},       
	
DOI={10.3389/fncir.2016.00023},      
	
ISSN={1662-5110},   
   
ABSTRACT={Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}

@article{Advanced,
    author = {Numenta},
    publisher = {Numenta},
    title = {Advanced NuPIC Programming},
    year = {2008}
}

@online{Principles,
        title = {Principles of Hierarchical Temporal Memory (HTM): Foundations of Machine Intelligence},
        year = {2014},
        month = {October},
        organization = {Youtube},
        author = {Numenta},
        url = {https://www.youtube.com/watch?v=6ufPpZDmPKA},
}

@article{ContinuousOnline,
author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},
journal = {Neural Computation},
volume = {28},
number = {11},
pages = {2474-2504},
year = {2016},
doi = {10.1162/NECO\_a\_00893},
    note ={PMID: 27626963},

URL = { 
        https://doi.org/10.1162/NECO_a_00893
    
},
eprint = { 
        https://doi.org/10.1162/NECO_a_00893
    
}
,
    abstract = { The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methodsâ€”autoregressive integrated moving average; feedforward neural networksâ€”time delay neural network and online sequential extreme learning machine; and recurrent neural networksâ€”long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams. }
}

@ARTICLE{TheHTMSpatialPooler,
  
AUTHOR={Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},   
	 
TITLE={The HTM Spatial Poolerâ€”A Neocortical Algorithm for Online Sparse Distributed Coding},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={11},      

PAGES={111},     
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncom.2017.00111},       
	
DOI={10.3389/fncom.2017.00111},      
	
ISSN={1662-5188},   
   
ABSTRACT={Hierarchical temporal memory (HTM) provides a theoretical framework that models several key computational principles of the neocortex. In this paper we analyze an important component of HTM, the HTM spatial pooler (SP). The SP models how neurons learn feedforward connections and form efficient representations of the input. It converts arbitrary binary input patterns into sparse distributed representations (SDRs) using a combination of competitive Hebbian learning rules and homeostatic excitability control. We describe a number of key properties of the spatial pooler, including fast adaptation to changing input statistics, improved noise robustness through learning, efficient use of cells and robustness to cell death. In order to quantify these properties we develop a set of metrics that can be directly computed from the spatial pooler outputs. We show how the properties are met using these metrics and targeted artificial simulations. We then demonstrate the value of the spatial pooler in a complete end-to-end real-world HTM system. We discuss the relationship with neuroscience and previous studies of sparse coding. The HTM spatial pooler represents a neurally inspired algorithm for learning sparse representations from noisy data streams in an online fashion.}
}

@book{OnIntelligence,
 author = {Hawkins, Jeff and Blakeslee, Sandra},
 title = {On Intelligence},
 year = {2004},
 isbn = {0805074562},
 publisher = {Times Books},
 address = {New York, NY, USA},
} 

@article{NAB,
  author    = {Alexander Lavin and
               Subutai Ahmad},
  title     = {Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly
               Benchmark},
  journal   = {CoRR},
  volume    = {abs/1510.03336},
  year      = {2015},
  url       = {http://arxiv.org/abs/1510.03336},
  archivePrefix = {arXiv},
  eprint    = {1510.03336},
  timestamp = {Mon, 13 Aug 2018 16:47:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LavinA15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


######################### Natural Computation ####################################

@inproceedings{Eberhart,
	doi = {10.1109/mhs.1995.494215},
	url = {https://doi.org/10.1109%2Fmhs.1995.494215},
	publisher = {{IEEE}},
	author = {R. Eberhart and J. Kennedy},
	title = {A new optimizer using particle swarm theory},
	booktitle = {{MHS}{\textquotesingle}95. Proceedings of the Sixth International Symposium on Micro Machine and Human Science}
}

@article{FundNatComp,
title = "Fundamentals of natural computing: an overview",
journal = "Physics of Life Reviews",
volume = "4",
number = "1",
pages = "1 - 36",
year = "2007",
issn = "1571-0645",
doi = "https://doi.org/10.1016/j.plrev.2006.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S1571064506000315",
author = "Leandro Nunes de Castro",
keywords = "Natural computing, Bio-inspired computing, Problem-solving, Novel computing paradigms",
abstract = "Natural computing is a terminology introduced to encompass three classes of methods: (1) those that take inspiration from nature for the development of novel problem-solving techniques; (2) those that are based on the use of computers to synthesize natural phenomena; and (3) those that employ natural materials (e.g., molecules) to compute. The main fields of research that compose these three branches are the artificial neural networks, evolutionary algorithms, swarm intelligence, artificial immune systems, fractal geometry, artificial life, DNA computing, and quantum computing, among others. This paper provides an overview of the fundamentals of natural computing, particularly the fields listed above, emphasizing the biological motivation, some design principles, their scope of applications, current research trends and open problems. The presentation is concluded with a discussion about natural computing, and when it should be used."
}

@Article{PSOReview,
  author="Banks, Alec and Vincent, Jonathan and Anyakoha, Chukwudi",
  title="A review of particle swarm optimization. Part I: background and development",
  journal="Natural Computing",
  year="2007",
  month="Dec",
  day="01",
  volume="6",
  number="4",
  pages="467--484",
  abstract="Particle Swarm Optimization (PSO), in its present form, has been in existence for roughly a decade, with formative research in related domains (such as social modelling, computer graphics, simulation and animation of natural swarms or flocks) for some years before that; a relatively short time compared with some of the other natural computing paradigms such as artificial neural networks and evolutionary computation. However, in that short period, PSO has gained widespread appeal amongst researchers and has been shown to offer good performance in a variety of application domains, with potential for hybridisation and specialisation, and demonstration of some interesting emergent behaviour. This paper aims to offer a compendious and timely review of the field and the challenges and opportunities offered by this welcome addition to the optimization toolbox. Part I discusses the location of PSO within the broader domain of natural computing, considers the development of the algorithm, and refinements introduced to prevent swarm stagnation and tackle dynamic environments. Part II considers current research in hybridisation, combinatorial problems, multicriteria and constrained optimization, and a range of indicative application areas.",
  issn="1572-9796",
  doi="10.1007/s11047-007-9049-5",
  url="https://doi.org/10.1007/s11047-007-9049-5"
}



######################### SDRs and Coding ################################################

@article{Properties,
  title={Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory},
  author={Ahmad, Subutai and Hawkins, Jeff},
  journal={arXiv preprint arXiv:1503.07469},
  year={2015}
}

@misc{RDSE,
  author = {Byrne, Fergal},
  title = {Random Distributed Scalar Encoder},
  howpublished = "\url{http://fergalbyrne.github.io/rdse.html}",
  year = {2014},
  month = {7},
  note = {Online; accessed 27-April-2018}
}

@misc{Dillon,
  author = {Dillon, A},
  title = {SDR Classifier},
  howpublished = {\url{http://hopding.com/sdr-classifier}},
  year = {2016},
  note = {Online; accessed 9-April-2018}
}

@article{Purdy,
  author    = {Scott Purdy},
  title     = {Encoding Data for {HTM} Systems},
  journal   = {CoRR},
  volume    = {abs/1602.05925},
  year      = {2016}
}

@article{Semantic,
  author    = {Francisco De Sousa Webber},
  title     = {Semantic Folding Theory And its Application in Semantic Fingerprinting},
  journal   = {CoRR},
  volume    = {abs/1511.08855},
  year      = {2015}
}

######################## Statsmodels ##########################################

@inproceedings{statsmodels,
  title={Statsmodels: Econometric and statistical modeling with python},
  author={Seabold, Skipper and Perktold, Josef},
  booktitle={9th Python in Science Conference},
  year={2010},
}
